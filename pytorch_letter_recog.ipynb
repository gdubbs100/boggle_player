{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: 'A', 12: 'B', 13: 'C', 14: 'D', 15: 'E', 16: 'F', 17: 'G', 18: 'H', 19: 'I', 20: 'J', 21: 'K', 22: 'L', 23: 'M', 24: 'N', 25: 'O', 26: 'P', 27: 'Q', 28: 'R', 29: 'S', 30: 'T', 31: 'U', 32: 'V', 33: 'W', 34: 'X', 35: 'Y', 36: 'Z', 37: 'a', 38: 'b', 39: 'c', 40: 'd', 41: 'e', 42: 'f', 43: 'g', 44: 'h', 45: 'i', 46: 'j', 47: 'k', 48: 'l', 49: 'm', 50: 'n', 51: 'o', 52: 'p', 53: 'q', 54: 'r', 55: 's', 56: 't', 57: 'u', 58: 'v', 59: 'w', 60: 'x', 61: 'y', 62: 'z'}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from itertools import chain\n",
    "chars = [string.digits,string.ascii_uppercase,string.ascii_lowercase]\n",
    "class_mapping = {(idx + 1):char for idx, char in enumerate(chain(*chars)) }\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 103.54\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "fixed_size = tuple((100, 100))\n",
    "image = cv.imread('./English/Img/GoodImg/Bmp/Sample012/img012-00025.png')\n",
    "# cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image = cv.resize(image, fixed_size)\n",
    "greyscale = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "print(greyscale[0,0], greyscale.mean())\n",
    "if greyscale[0,0] >= greyscale.mean():\n",
    "    \n",
    "    greyscale[greyscale >= greyscale.mean()] = 0\n",
    "    greyscale[greyscale < greyscale.mean()] = 1\n",
    "else:\n",
    "    \n",
    "    greyscale[greyscale < greyscale.mean()] = 0\n",
    "    greyscale[greyscale >= greyscale.mean()] = 1\n",
    "    \n",
    "\n",
    "    \n",
    "plt.imshow(greyscale, cmap = 'Greys')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinaryImage(image, fixed_size = tuple((100,100))):\n",
    "    \"\"\"Converts an image to a black/white binary image.\n",
    "    Examines the corner to determine which cells to convert to 1 and which to 0.\n",
    "    Aims to make the letters black - but doesn't always work!!\"\"\"\n",
    "    \n",
    "    new_image = cv.resize(image, fixed_size)\n",
    "    greyscale = cv.cvtColor(new_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    if greyscale[0,0] >= greyscale.mean():\n",
    "        greyscale[greyscale < greyscale.mean()] = 1.0\n",
    "        greyscale[greyscale >= greyscale.mean()] = 0.0\n",
    "        \n",
    "    else:\n",
    "        greyscale[greyscale < greyscale.mean()] = 0.0\n",
    "        greyscale[greyscale >= greyscale.mean()] = 1.0\n",
    "        \n",
    "    return greyscale.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "raw_dir = './English/Img/GoodImg/Bmp'\n",
    "X = []\n",
    "labels = [] # convert to y with encoding later\n",
    "for _file in os.listdir(raw_dir):\n",
    "    key = int(_file[-3:])\n",
    "    curr_label = class_mapping[key]\n",
    "    if key in range(11,37):\n",
    "        for _sample in os.listdir(os.path.join(raw_dir,_file)):\n",
    "            image = cv.imread(os.path.join(raw_dir, _file,_sample))\n",
    "            bin_image = getBinaryImage(image)\n",
    "            X.append(bin_image)\n",
    "            labels.append(curr_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# batch size\n",
    "bs = 4\n",
    "\n",
    "# encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "y = le.transform(labels)\n",
    "# split test / train\n",
    "X_train, X_test, y_train, y_test = map(\n",
    "    torch.tensor, \n",
    "    train_test_split(\n",
    "        X, \n",
    "        y,\n",
    "        test_size=0.2, \n",
    "        random_state=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "# create Tensor datasets and dataloaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size= bs)\n",
    "test_dl = DataLoader(test_ds, batch_size= bs) # not sure why *2??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple nn to start\n",
    "\n",
    "class logReg(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10000, 36)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "net = logReg()\n",
    "criterion = nn.CrossEntropyLoss() # for multi-class classification\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function to play with hyperparams\n",
    "def Train(net, epochs, trainloader, criterion, optimizer):\n",
    "    # train the network\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients (still not sure what this is really?)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print stats\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99: # print every 2000 minibatches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0 # why reset this? - its an average loss for last 2000 mini batches\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "def Eval(net, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 3.374\n",
      "[1,   200] loss: 3.272\n",
      "[1,   300] loss: 3.218\n",
      "[1,   400] loss: 3.172\n",
      "[1,   500] loss: 3.138\n",
      "[1,   600] loss: 3.088\n",
      "[1,   700] loss: 3.116\n",
      "[1,   800] loss: 3.098\n",
      "[1,   900] loss: 3.075\n",
      "[1,  1000] loss: 3.102\n",
      "[1,  1100] loss: 3.038\n",
      "[1,  1200] loss: 3.041\n",
      "[1,  1300] loss: 3.021\n",
      "[1,  1400] loss: 3.038\n",
      "[1,  1500] loss: 3.018\n",
      "[2,   100] loss: 3.015\n",
      "[2,   200] loss: 3.024\n",
      "[2,   300] loss: 2.991\n",
      "[2,   400] loss: 3.003\n",
      "[2,   500] loss: 2.992\n",
      "[2,   600] loss: 2.956\n",
      "[2,   700] loss: 2.980\n",
      "[2,   800] loss: 2.979\n",
      "[2,   900] loss: 2.970\n",
      "[2,  1000] loss: 2.994\n",
      "[2,  1100] loss: 2.948\n",
      "[2,  1200] loss: 2.966\n",
      "[2,  1300] loss: 2.947\n",
      "[2,  1400] loss: 2.971\n",
      "[2,  1500] loss: 2.963\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "Train(net, 2, train_dl, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 47 %\n"
     ]
    }
   ],
   "source": [
    "Eval(net, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those were pretty garbage. lets try adding some layers\n",
    "class bigNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10000, 5000)\n",
    "        self.linear2 = nn.Linear(5000, 1000)\n",
    "        self.linear3 = nn.Linear(1000, 36)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 3.431\n",
      "[1,   200] loss: 3.088\n",
      "[1,   300] loss: 2.717\n",
      "[1,   400] loss: 2.364\n",
      "[1,   500] loss: 2.259\n",
      "[1,   600] loss: 2.071\n",
      "[1,   700] loss: 1.978\n",
      "[2,   100] loss: 1.954\n",
      "[2,   200] loss: 1.948\n",
      "[2,   300] loss: 1.847\n",
      "[2,   400] loss: 1.798\n",
      "[2,   500] loss: 1.842\n",
      "[2,   600] loss: 1.680\n",
      "[2,   700] loss: 1.668\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = bigNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # for multi-class classification\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9) # decrease learning rate\n",
    "\n",
    "Train(net, 2, train_dl, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 57 %\n"
     ]
    }
   ],
   "source": [
    "Eval(net, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 20)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(1, 4, 10)\n",
    "        # self.fc1 = nn.Linear(int(80 * 80 / 4), 800)\n",
    "        self.fc1 = nn.Linear(int(4 * 15 * 15), 800)\n",
    "        self.fc2 = nn.Linear(800, 200)\n",
    "        self.fc3 = nn.Linear(200, 36)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, 100,100)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 3.563\n",
      "[1,   200] loss: 3.150\n",
      "[1,   300] loss: 2.599\n",
      "[1,   400] loss: 2.187\n",
      "[1,   500] loss: 1.827\n",
      "[1,   600] loss: 1.681\n",
      "[1,   700] loss: 1.568\n",
      "[1,   800] loss: 1.431\n",
      "[1,   900] loss: 1.252\n",
      "[1,  1000] loss: 1.344\n",
      "[2,   100] loss: 1.266\n",
      "[2,   200] loss: 1.100\n",
      "[2,   300] loss: 1.199\n",
      "[2,   400] loss: 1.173\n",
      "[2,   500] loss: 1.116\n",
      "[2,   600] loss: 1.047\n",
      "[2,   700] loss: 1.024\n",
      "[2,   800] loss: 0.972\n",
      "[2,   900] loss: 0.920\n",
      "[2,  1000] loss: 1.030\n",
      "[3,   100] loss: 1.005\n",
      "[3,   200] loss: 0.851\n",
      "[3,   300] loss: 0.987\n",
      "[3,   400] loss: 0.952\n",
      "[3,   500] loss: 0.904\n",
      "[3,   600] loss: 0.946\n",
      "[3,   700] loss: 0.913\n",
      "[3,   800] loss: 0.894\n",
      "[3,   900] loss: 0.799\n",
      "[3,  1000] loss: 0.885\n",
      "[4,   100] loss: 0.914\n",
      "[4,   200] loss: 0.743\n",
      "[4,   300] loss: 0.907\n",
      "[4,   400] loss: 0.832\n",
      "[4,   500] loss: 0.811\n",
      "[4,   600] loss: 0.894\n",
      "[4,   700] loss: 0.785\n",
      "[4,   800] loss: 0.782\n",
      "[4,   900] loss: 0.709\n",
      "[4,  1000] loss: 0.819\n",
      "[5,   100] loss: 0.851\n",
      "[5,   200] loss: 0.674\n",
      "[5,   300] loss: 0.799\n",
      "[5,   400] loss: 0.781\n",
      "[5,   500] loss: 0.844\n",
      "[5,   600] loss: 0.823\n",
      "[5,   700] loss: 0.719\n",
      "[5,   800] loss: 0.658\n",
      "[5,   900] loss: 0.791\n",
      "[5,  1000] loss: 0.809\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = convNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # for multi-class classification\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9, weight_decay=0.05) # decrease learning rate\n",
    "\n",
    "Train(net, 5, train_dl, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 72 %\n"
     ]
    }
   ],
   "source": [
    "Eval(net, test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: A     is 96.8 %\n",
      "Accuracy for class: B     is 36.0 %\n",
      "Accuracy for class: C     is 87.8 %\n",
      "Accuracy for class: D     is 78.4 %\n",
      "Accuracy for class: E     is 89.0 %\n",
      "Accuracy for class: F     is 44.4 %\n",
      "Accuracy for class: G     is 90.3 %\n",
      "Accuracy for class: H     is 77.1 %\n",
      "Accuracy for class: I     is 83.0 %\n",
      "Accuracy for class: J     is 53.3 %\n",
      "Accuracy for class: K     is 87.0 %\n",
      "Accuracy for class: L     is 86.8 %\n",
      "Accuracy for class: M     is 83.9 %\n",
      "Accuracy for class: N     is 90.1 %\n",
      "Accuracy for class: O     is 85.5 %\n",
      "Accuracy for class: P     is 88.5 %\n",
      "Accuracy for class: Q     is 11.1 %\n",
      "Accuracy for class: R     is 81.4 %\n",
      "Accuracy for class: S     is 89.8 %\n",
      "Accuracy for class: T     is 84.6 %\n",
      "Accuracy for class: U     is 52.0 %\n",
      "Accuracy for class: V     is 93.8 %\n",
      "Accuracy for class: W     is 70.0 %\n",
      "Accuracy for class: X     is 69.2 %\n",
      "Accuracy for class: Y     is 58.3 %\n",
      "Accuracy for class: Z     is 80.0 %\n"
     ]
    }
   ],
   "source": [
    "# check against\n",
    "correct_pred = {v: 0 for k,v in class_mapping.items() if k in range(11, 37)}\n",
    "total_pred = {v: 0 for k,v in class_mapping.items() if k in range(11, 37)}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        outputs = net(images.float())\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect correct pred for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "\n",
    "            if label == prediction:\n",
    "                correct_pred[le.inverse_transform([label])[0]] += 1\n",
    "            total_pred[le.inverse_transform([label])[0]] += 1\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try tensorboard\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "writer = SummaryWriter('runs/letter_recog_experiment1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.view(4, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dl)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(images)#.mean(dim = 0, dtype = float)\n",
    "\n",
    "def showImg(img):\n",
    "    img = img.mean(dim = 0, dtype = float)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg.reshape(400, 100), (1,0)), cmap = 'Greys')\n",
    "\n",
    "# plt.imshow(images.view( 400, 100), cmap = 'Greys')\n",
    "# img_grid.shape\n",
    "writer.add_image('Letter outputs', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images.float())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10000])\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "N = 10\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(train_ds.tensors[0], train_ds.tensors[1], n = N)\n",
    "\n",
    "# # get the class labels for each image\n",
    "class_labels = [le.inverse_transform([lab])[0][0] for lab in labels]\n",
    "\n",
    "# # log embeddings\n",
    "features = images.view(-1, 100 * 100)\n",
    "print(features.shape)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1).view(N, 1, 100, 100))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 100, 100])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.unsqueeze(1).view(100, 1, 100, 100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:32<00:00, 819174.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 93343.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:05<00:00, 786791.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# # get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# # log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "print(features.shape)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.unsqueeze(1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35643e8013a8986bd707b2979c5c936bcf5bc6379996007802b723641253d1ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
